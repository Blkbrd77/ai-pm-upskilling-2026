{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Comparison\n",
    "\n",
    "**Objective:** Compare ensemble methods for project outcome prediction.\n",
    "\n",
    "**Models to Compare:**\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- (Optional) XGBoost\n",
    "\n",
    "**Target:** Project success (binary) or cost overrun (regression)\n",
    "\n",
    "---\n",
    "\n",
    "## Milestones\n",
    "- [ ] Data loaded (+10 pts)\n",
    "- [ ] Data preprocessed (+10 pts)\n",
    "- [ ] Random Forest trained (+10 pts)\n",
    "- [ ] Gradient Boosting trained (+10 pts)\n",
    "- [ ] AdaBoost trained (+10 pts)\n",
    "- [ ] Comparison table created (+10 pts)\n",
    "- [ ] Feature importance analyzed (+10 pts)\n",
    "- [ ] Notebook complete (+25 pts)\n",
    "\n",
    "**Total possible: 95 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and prep data\n",
    "# df = pd.read_csv('data.csv')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# rf_score = accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Gradient Boosting\n",
    "# gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "# gb.fit(X_train, y_train)\n",
    "# gb_score = accuracy_score(y_test, gb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train AdaBoost\n",
    "# ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "# ada.fit(X_train, y_train)\n",
    "# ada_score = accuracy_score(y_test, ada.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison table\n",
    "# results = pd.DataFrame({\n",
    "#     'Model': ['Random Forest', 'Gradient Boosting', 'AdaBoost'],\n",
    "#     'Accuracy': [rf_score, gb_score, ada_score],\n",
    "#     'CV Mean': [rf_cv, gb_cv, ada_cv]\n",
    "# })\n",
    "# results.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot feature importance from best model\n",
    "# importances = best_model.feature_importances_\n",
    "# plt.barh(feature_names, importances)\n",
    "# plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bar chart comparing models\n",
    "# models = ['RF', 'GB', 'Ada']\n",
    "# scores = [rf_score, gb_score, ada_score]\n",
    "# plt.bar(models, scores)\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Ensemble Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the best performing model\n",
    "# joblib.dump(best_model, '../../app/models/ensemble_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "**Model Rankings:**\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "**Key Features:**\n",
    "- \n",
    "\n",
    "**Recommendation for Flask App:**\n",
    "- \n",
    "\n",
    "**Badge Earned:** :deciduous_tree: Ensemble Expert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
